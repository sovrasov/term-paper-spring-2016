\section{Введение}
//здесь про важность задач глобаьной оптимизации
Задачи глобальной оптимизации встречаются в . Сложность этих задач экспоненциально растёт в зависимости от размерности пространства поиска, поэтому для решения существенно многомерных задач требуются
суперкомпьютерные вычисления.
\par
В настоящее время на кафедре МОиСТ активно ведётся разработка программной системы для глобальной оптимизации функций многих вещественных переменных ExaMin.
Эта система включает в себя последние теоретические разработки, сделанные на кафедре в этой сфере, в том числе и блочную многошаговую схему редукции размерности \cite{blockNested}.
Отличительной чертой ситемы является то, что, она может работать как на CPU, так на разных типах ускорителей вычислений с высокой степенью параллельности (XeonPhi, GPU Nvidia) \cite{examinArtcle, examinPhiArtcle}.
\par
В данной работе будут описаны некеторые улучшения, внесённые в систему, и предварительные исследования, проведённые перед их внедрением.
\section{Алгоритм глобального поиска}
 Для дальнейшего изложения потребуется описание метода глобальной оптимизации, используемого в системе ExaMin. Многомерные задачи сводятся к одномерным с помощью различных схем редукции размерноти,
 поэтому можно рассматривать минимизацию одномерной функции  \(f(x), x\in[0,1]\), удовлетворяющей условию Гёльдера.
\par
Рассматриваемый алгоритм решения данной задачи предполагает построение последовательности точек \(x_k\), в которых вычисляются значения минимизируемой функции \(z_k = f(x_k)\).
Процесс вычисления значения функции (включающий в себя построение образа \(y_k=y(x_k))\) будем называть испытанием, а пару \((x_k,z_k)\) --- результатом испытания.
Множество пар \(\{(x_k,z_k)\}, 1\leqslant k\leqslant n\) составляет поисковую информацию, накопленную методом после проведения \(n\) шагов.
\par
На первой итерации метода испытание проводится в произвольной внутренней точке \(x_1\) интервала \([0;1]\). Пусть выполнено \(k\geqslant 1\) итераций метода,
в процессе которых были проведены испытания в \(k\) точках \(x_i, 1\leqslant i\leqslant k\). Тогда точкa \(x^{k+1}\) поисковых испытаний следующей \((k+1)\)-ой
итерации определяются в соответствии с правилами:
\par
Шаг 1. Перенумеровать точки множества \(X_k=\{x^1,\dotsc,x^k\}\cup\{0\}\cup\{1\}\), которое включает в себя граничные точки интервала \([0,1]\), а также точки предшествующих испытаний, нижними индексами в порядке увеличения значений координаты, т.е.
\begin{displaymath}
0=x_0<x_1<\dotsc<x_{k+1}=1
\end{displaymath}
\par
Шаг 2. Полагая \(z_i=f(x_i),1\leqslant i\leqslant k\), вычислить величины
\begin{equation}
\label{step2}
\mu=\max_{1\leqslant i\leqslant k}\dfrac{|z_i-z_{i-1}|}{\Delta_i},
\begin{matrix}
    M =
    \left\{
    \begin{matrix}
    r\mu,\mu>0 \\
    1,\mu=0
    \end{matrix} \right.
    \end{matrix}
\end{equation}
где \(r\) является заданным параметром метода, а \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
\par
Шаг 3. Для каждого интервала \((x_{i-1},x_i),1\leqslant i\leqslant k+1\), вычислить характеристику в соответствии с формулами
\begin{equation}
\label{step3_1}
R(1)=2\Delta_1-4\dfrac{z_1}{M},R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M}
\end{equation}
\begin{equation}
\label{step3_2}
R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1
\end{equation}
\par
Шаг 4. Выбрать наибольшую характеристику:
\begin{equation}
\label{step4}
t=\argmax_{1\leqslant i \leqslant k+1}R(i)
\end{equation}
\par
Шаг 5. Провести очередное испытания в точке \(x_{k+1}\), вычисленной по формулам
\begin{displaymath}
x_{k+1}=\dfrac{x_{t}+x_{t-1}}{2},t=1,t=k+1
\end{displaymath}
\begin{equation}
\label{step5}
x_{k+1}=\dfrac{x_{t}+x_{t-1}}{2}-\sign(z_{t}-z_{t-1})\dfrac{1}{2r}\left[\dfrac{|z_{t}-z_{t-1}|}{\mu}\right]^N,1<t<k+1
\end{equation}
\par
Алгоритм прекращает работу, если выполняется условие \(\Delta_{t}\leqslant \varepsilon\), где \(\varepsilon>0\) есть заданная точность. В качестве оценки глобально-оптимального решения задачи  выбираются значения
\begin{equation}
f_k^*=\min_{1\leqslant i \leqslant k}f(x_i), x_k^*=\argmin_{1\leqslant i \leqslant k}f(x_i)
\end{equation}
\par
Подробнее метод и теорема о его сходимости описаны в \cite{strOptBook}.
\subsection{Сравнение методов оптимизации}
Существует несколько критериев оптимальности алгоритмов поиска (минимаксный, критерий одношаговой оптимальности), но большинстве случаев представляет интерес
сравнение методов по среднему результату, достижимому на конкретном подклассе липшицевых функций. Достоинством такого подхода является то, что средний показатель можно оценить
по конечной случайной выборке задач, используя методы математической статистики.
\par
В качестве оценки эффективности алгоритма будем использовать, операционную характеристику, которая определяется множеством точек на плоскости \((K, P)\),
где \(K\) – среднее число поисковых испытаний, предшествующих выполнению условия остановки при минимизации функции из данного класса, а \(P\) – статистическая вероятность того,
что к моменту остановки глобальный экстремум будет найден с заданной точностью. Если при выбранном \(K\) операционная характеристика одного метода лежит выше характеристики другого,
то это значит, что при фиксированных затратах на поиск первый метод найдёт решение с большей статистической вероятностью. Если же зафиксировать некоторое значение \(P\), и
характеристика одного метода лежит левее характеристики другого, то первый метод требует меньше затрат на достижение той же надёжности.
\subsection{Класс тестовых задач GKLS}
Для сравнения алгоритмов глобального поиска в смысле операционной характеристики требуется иметь некоторое множество тестовых задач.
Генератор задач GKLS, описанный в \cite{gklsBook} позволяет получить такое множество задач с заренее известными свойствами.
В данной работе используются два класса, сгенерированные GKLS: 4d Simple и 5d Simple, параметры которых также описаны в \cite{gklsBook}. Функции рассматриваемых классов являются непрерывно
дифференцируемыми и имеют 10 локальных минимумов, один из которых является глобальным.
